import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import json
from pathlib import Path
import warnings
import re
import argparse

# ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªè¡¨ç¤ºè¨­å®š (å¿…è¦ã«å¿œã˜ã¦)
# from matplotlib import rcParams
# rcParams['font.family'] = 'sans-serif'
# rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic']

# è­¦å‘Šã‚’éè¡¨ç¤ºã«ã™ã‚‹
warnings.simplefilter('ignore', FutureWarning)

COLOR_PALETTE = {
    "XGBoost": "#1f77b4",       # ãƒ–ãƒ«ãƒ¼
    "RandomForest": "#ff7f0e",  # ã‚ªãƒ¬ãƒ³ã‚¸
    "Random": "#2ca02c"         # ç·‘ (è¿½åŠ )
}
## ------------------------------------------------------
## è¨­å®š
## ------------------------------------------------------
# æ¯”è¼ƒã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¾æ›¸ã§è¨­å®šã—ã¾ã™ã€‚
# ã‚­ãƒ¼: ãƒ¢ãƒ‡ãƒ«å (ã‚°ãƒ©ãƒ•ã®å‡¡ä¾‹ãªã©ã§ä½¿ç”¨)
# ãƒãƒªãƒ¥ãƒ¼: å¯¾å¿œã™ã‚‹çµæœãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
REPO_ROOT = Path(__file__).resolve().parents[2]
RESULTS_ROOT = REPO_ROOT / "datasets" / "model_outputs" 
# /work/riku-ka/vuljit/datasets/model_outputs
BASE_DIRS = {
    "XGBoost": RESULTS_ROOT / "xgboost",
    "RandomForest": RESULTS_ROOT / "random_forest",
    "Random": RESULTS_ROOT / "random",
}


## ------------------------------------------------------
## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
## ------------------------------------------------------
def load_experiment_data(base_dir: Path, project: str, exp_number: int) -> tuple[pd.DataFrame | None, dict | None]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€å®Ÿé¨“ç•ªå·ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
    """
    project_path = Path(project)
    importance_path = base_dir / project_path / f"exp{exp_number}_importances.csv"
    metrics_path = base_dir / project_path / f"exp{exp_number}_metrics.json"

    importance_df, metrics_dict = None, None

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿èª­ã¿è¾¼ã‚€
    if importance_path.exists():
        importance_df = pd.read_csv(importance_path)
        if 'importance' in importance_df.columns:
            importance_df['importance'] = importance_df['importance'].clip(lower=0)

    if metrics_path.exists():
        with open(metrics_path, 'r') as f:
            metrics_dict = json.load(f)

    return importance_df, metrics_dict


## ------------------------------------------------------
## ãƒ¢ãƒ‡ãƒ«é–“ã®æ€§èƒ½ã‚’æ¯”è¼ƒãƒ»å¯è¦–åŒ–ã™ã‚‹é–¢æ•° (â˜…æ”¹è‰¯ç‰ˆ)
## ------------------------------------------------------
def visualize_per_model_importance(all_metrics_df: pd.DataFrame, all_importances_df: pd.DataFrame, exp_num: int, num_projects: int):
    """
    è©•ä¾¡æŒ‡æ¨™ã¯ãƒ¢ãƒ‡ãƒ«é–“ã§æ¯”è¼ƒã—ã€ç‰¹å¾´é‡é‡è¦åº¦ã¯ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å€‹åˆ¥ã®ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–ã™ã‚‹é–¢æ•°
    """
    model_names = all_metrics_df['model'].unique()
    print(f"\n--- Visualizing Performance for Exp {exp_num} ({num_projects} projects) across {len(model_names)} models ---")

    # 1. è©•ä¾¡æŒ‡æ¨™ã®åˆ†å¸ƒã‚’ãƒ¢ãƒ‡ãƒ«é–“ã§æ¯”è¼ƒãƒ»å¯è¦–åŒ– (å¤‰æ›´ãªã—)
    plt.figure(figsize=(18, 9))

    median_order = all_metrics_df.groupby('Metric')['Value'].median().sort_values(ascending=False).index

    sns.violinplot(x='Metric', y='Value', hue='model', data=all_metrics_df, order=median_order,
                   palette=COLOR_PALETTE, inner='box', linewidth=1.5, saturation=0.8)

    plt.title(f'Metrics Comparison (Violin Plot) for Exp {exp_num} (across {num_projects} projects)', fontsize=18, weight='bold')
    plt.xlabel('Metric', fontsize=14)
    plt.ylabel('Score', fontsize=14)
    # â–¼ å¤‰æ›´: Yè»¸ç¯„å›²ã‚’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„ã«æ±ºå®šï¼ˆè² ã®MCCã‚‚å¯è¦–åŒ–ï¼‰
    min_val = all_metrics_df['Value'].min(skipna=True)
    max_val = all_metrics_df['Value'].max(skipna=True)
    y_min = 0 if pd.isna(min_val) or min_val >= 0 else min(min_val * 1.05, -1.0)
    y_max = 1.0 if pd.isna(max_val) else max(1.0, max_val * 1.05)
    plt.ylim(y_min, y_max)
    # plt.ylim(0, 1.05)  # â† å›ºå®šç¯„å›²ã¯å‰Šé™¤
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.legend(title='Model', fontsize=12)
    plt.tight_layout()
    # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
    metrics_filename = f"exp{exp_num}_metrics_comparison.png"
    plt.savefig(metrics_filename)
    print(f"  ğŸ“ˆ Metrics comparison plot saved as: {metrics_filename}")
    plt.show()
    plt.close()

    # 2. ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†å¸ƒã‚’ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å€‹åˆ¥ã«å¯è¦–åŒ– (â˜…å¤‰æ›´ç‚¹)
    print("  - Generating separate feature importance plots for each model...")
    for model_name in model_names:
        plt.figure(figsize=(14, 12))

        # å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã®é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
        model_importances_df = all_importances_df[all_importances_df['model'] == model_name]
        
        if model_importances_df.empty:
            continue

        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«Top 20ã®ç‰¹å¾´é‡ã‚’æ±ºå®š
        median_importances = model_importances_df.groupby('feature')['importance'].median().sort_values(ascending=False)
        top_20_features = median_importances.head(20).index
        top_features_df = model_importances_df[model_importances_df['feature'].isin(top_20_features)]
        
        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”» (hueã¯ä¸è¦)
        sns.violinplot(x='importance', y='feature', data=top_features_df, order=top_20_features,
                       orient='h', color='skyblue', inner='box', linewidth=1.5, saturation=0.8)

        plt.title(f'Feature Importance ({model_name}) for Exp {exp_num} - Top 20', fontsize=18, weight='bold')
        plt.xlabel('Importance', fontsize=14)
        plt.ylabel('Feature', fontsize=12)
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        # ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚ã¦ä¿å­˜
        importance_filename = f"exp{exp_num}_feature_importance_{model_name}.png"
        plt.savefig(importance_filename)
        print(f"  ğŸ“ˆ Feature importance plot for '{model_name}' saved as: {importance_filename}")
        plt.show()
        plt.close()


## ------------------------------------------------------
## ä¸Šä½Nä»¶ã®æ€§èƒ½å‡ºåŠ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
## ------------------------------------------------------
def export_top_n_performance(final_metrics_df: pd.DataFrame, exp_num: int, out_dir: Path, top_n: int, metric_name: str, make_plots: bool = False):
    """
    æŒ‡å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹(metric_name)ã§ã€ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸Šä½Nä»¶ã‚’æŠ½å‡ºã—ã¦CSVä¿å­˜ã€‚
    å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æ£’ã‚°ãƒ©ãƒ•ã‚‚ä¿å­˜ã—ã¾ã™ã€‚

    å‡ºåŠ›: 
      - exp{exp_num}_top{N}_{metric}_by_model.csv
      - (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) exp{exp_num}_top{N}_{metric}_{model}.png
    """
    if top_n is None or top_n <= 0:
        return

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åä¸€è‡´ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’ç„¡è¦–ï¼‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    metric_mask = final_metrics_df['Metric'].str.lower() == str(metric_name).lower()
    df_metric = final_metrics_df.loc[metric_mask].copy()
    if df_metric.empty:
        print(f"  âš ï¸ No rows found for metric '{metric_name}'. Skipping top-{top_n} export.")
        return

    # NaNã‚’é™¤å¤–ã—ã€Valueé™é †ï¼ˆé«˜ã„æ–¹ãŒè‰¯ã„å‰æï¼‰ã§ãƒ©ãƒ³ã‚¯ä»˜ã‘
    df_metric = df_metric.dropna(subset=['Value'])
    if df_metric.empty:
        print(f"  âš ï¸ All values for '{metric_name}' are NaN. Skipping top-{top_n} export.")
        return

    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ä¸Šä½Nä»¶ã‚’æŠ½å‡º
    top_rows = []
    for model_name, g in df_metric.groupby('model'):
        g_sorted = g.sort_values('Value', ascending=False).head(top_n).copy()
        if g_sorted.empty:
            continue
        g_sorted['Rank'] = range(1, len(g_sorted) + 1)
        # åˆ—ã®ä¸¦ã³ã‚’æ•´ãˆã‚‹
        g_sorted = g_sorted[['model', 'Rank', 'project', 'Metric', 'Value']]
        top_rows.append(g_sorted)

    if not top_rows:
        print(f"  âš ï¸ No top-{top_n} rows computed for metric '{metric_name}'.")
        return

    top_df = pd.concat(top_rows, ignore_index=True)
    csv_path = out_dir / f"exp{exp_num}_top{top_n}_{metric_name}_by_model.csv"
    top_df.to_csv(csv_path, index=False)
    print(f"  âœ… Saved top-{top_n} by model for '{metric_name}' to: {csv_path.name}")

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒãƒ¼ã‚°ãƒ©ãƒ•å‡ºåŠ›
    if make_plots:
        for model_name, g in top_df.groupby('model'):
            if g.empty:
                continue
            plt.figure(figsize=(12, max(4, 0.5 * len(g))))
            g_sorted = g.sort_values('Value', ascending=False)
            plt.barh(g_sorted['project'], g_sorted['Value'], color=COLOR_PALETTE.get(model_name, '#888888'))
            plt.gca().invert_yaxis()  # ä¸Šä½ã‚’ä¸Šã«
            plt.xlabel(metric_name)
            plt.ylabel('Project')
            plt.title(f"Top {len(g_sorted)} {metric_name} â€” {model_name} (Exp {exp_num})")
            plt.tight_layout()
            fig_path = out_dir / f"exp{exp_num}_top{top_n}_{metric_name}_{model_name}.png"
            plt.savefig(fig_path, dpi=150)
            print(f"  ğŸ“ˆ Saved plot: {fig_path.name}")
            plt.close()


## ------------------------------------------------------
## é™½æ€§(is_vcc=True)æ—¥æ•° ä¸Šä½Nä»¶ã®æ€§èƒ½å‡ºåŠ› + å¯è¦–åŒ–
## ------------------------------------------------------
def _find_daily_csv_for_project(project: str, valid_models: list[str]) -> Path | None:
    """ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã—ãªã„æ—¥åˆ¥é›†è¨ˆCSVã‚’ã€å­˜åœ¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å„ªå…ˆé †ã§å–å¾—"""
    preference = ["XGBoost", "RandomForest", "Random"]
    project_path = Path(project)
    for model_name in preference:
        if model_name not in valid_models:
            continue
        project_root = BASE_DIRS[model_name] / project_path
        if not project_root.exists():
            continue
        candidates = sorted(project_root.glob("*_daily_aggregated_metrics_with_predictions.csv"))
        if candidates:
            return candidates[0]
    return None


def _count_positive_days(csv_path: Path) -> int:
    """is_vcc=True ã®è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    try:
        df = pd.read_csv(csv_path, usecols=["is_vcc"])
    except Exception:
        df = pd.read_csv(csv_path)
        if "is_vcc" not in df.columns:
            return 0
    s = df["is_vcc"]
    if s.dtype == bool:
        return int(s.sum())
    return int(s.astype(str).str.strip().str.lower().isin(["true", "1", "t", "yes"]).sum())


def export_top_by_positive_days(final_metrics_df: pd.DataFrame,
                                exp_num: int,
                                out_dir: Path,
                                projects: list[str],
                                valid_models: list[str],
                                top_n: int,
                                metric_name: str | None = None,
                                make_plots: bool = False):
    """
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®é™½æ€§(is_vcc=True)æ—¥æ•°ã®å¤šã„é †ã«ä¸Šä½Nä»¶ã‚’æŠ½å‡ºã—ã€
    ãã‚Œã‚‰ã®æ€§èƒ½ï¼ˆfinal_metrics_df ã«å«ã¾ã‚Œã‚‹å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ä»»æ„ã§ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã‚’CSVä¿å­˜ã€‚

    å‡ºåŠ›:
      - exp{exp_num}_top{N}_by_positive_days_all_metrics.csv
      - exp{exp_num}_top{N}_by_positive_days_{metric}.csv (metric_name æŒ‡å®šæ™‚)
      - (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) exp{exp_num}_top{N}_by_positive_days_{metric}_{model}.png
    """
    if top_n is None or top_n <= 0:
        return

    pos_counts = []
    for proj in projects:
        p = _find_daily_csv_for_project(proj, valid_models)
        if p is None:
            continue
        try:
            cnt = _count_positive_days(p)
        except Exception:
            cnt = 0
        pos_counts.append((proj, cnt))

    if not pos_counts:
        print("  âš ï¸ No positive-day counts could be computed (daily CSV not found). Skipping.")
        return

    pos_df = pd.DataFrame(pos_counts, columns=["project", "positive_days"]).sort_values("positive_days", ascending=False)
    top_projects = pos_df.head(top_n)["project"].tolist()

    # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§å‡ºåŠ›ï¼ˆlongå½¢å¼ã®ã¾ã¾ã€positive_daysåˆ—ã‚’ä»˜ä¸ï¼‰
    all_metrics_subset = final_metrics_df[final_metrics_df["project"].isin(top_projects)].copy()
    all_metrics_subset = all_metrics_subset.merge(pos_df, on="project", how="left")
    all_metrics_subset = all_metrics_subset.sort_values(["positive_days", "model", "Metric"], ascending=[False, True, True])

    csv_all = out_dir / f"exp{exp_num}_top{len(top_projects)}_by_positive_days_all_metrics.csv"
    all_metrics_subset.to_csv(csv_all, index=False)
    print(f"  âœ… Saved positives-based top list (all metrics): {csv_all.name}")

    # ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«çµã£ãŸå‡ºåŠ› + ãƒ—ãƒ­ãƒƒãƒˆ
    if metric_name:
        met_mask = all_metrics_subset["Metric"].str.lower() == str(metric_name).lower()
        met_df = all_metrics_subset.loc[met_mask].copy()
        csv_metric = out_dir / f"exp{exp_num}_top{len(top_projects)}_by_positive_days_{metric_name}.csv"
        met_df.to_csv(csv_metric, index=False)
        print(f"  âœ… Saved positives-based top list for metric '{metric_name}': {csv_metric.name}")

        if make_plots and not met_df.empty:
            for model_name, g in met_df.groupby('model'):
                if g.empty:
                    continue
                plt.figure(figsize=(12, max(4, 0.5 * len(g))))
                g_sorted = g.sort_values(["positive_days", "Value"], ascending=[False, False])
                labels = [f"{proj} ({pdays})" for proj, pdays in zip(g_sorted['project'], g_sorted['positive_days'])]
                plt.barh(labels, g_sorted['Value'], color=COLOR_PALETTE.get(model_name, '#888888'))
                plt.gca().invert_yaxis()
                plt.xlabel(metric_name)
                plt.ylabel('Project (positive days)')
                plt.title(f"Top {len(g_sorted)} by positives â€” {model_name} â€” {metric_name} (Exp {exp_num})")
                plt.tight_layout()
                fig_path = out_dir / f"exp{exp_num}_top{len(top_projects)}_by_positive_days_{metric_name}_{model_name}.png"
                plt.savefig(fig_path, dpi=150)
                print(f"  ğŸ“ˆ Saved plot: {fig_path.name}")
                plt.close()

## ------------------------------------------------------
## ãƒ¡ã‚¤ãƒ³å‡¦ç† (å¯è¦–åŒ–é–¢æ•°ã®å‘¼ã³å‡ºã—å…ˆã‚’å¤‰æ›´)
## ------------------------------------------------------
def main():
    """
    ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œé–¢æ•°
    """
    print("--- Starting Experiment Analysis ---")

    # å¼•æ•°
    parser = argparse.ArgumentParser(description='Compare model results and optionally export top-N performance.')
    parser.add_argument('--top-n', type=int, default=0, help='ä¸Šä½Nä»¶ã®æ€§èƒ½ã‚’å‡ºåŠ›ï¼ˆ0ã§ç„¡åŠ¹ï¼‰')
    parser.add_argument('--top-metric', type=str, default='MCC', help='ä¸Šä½æŠ½å‡ºã«ç”¨ã„ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆä¾‹: MCC, AUC_ROC, F1-Score ãªã©ï¼‰')
    parser.add_argument('--plot-top', action='store_true', help='ä¸Šä½Nä»¶ã®æ£’ã‚°ãƒ©ãƒ•ã‚‚ä¿å­˜ã™ã‚‹')
    parser.add_argument('--top-by-positives', type=int, default=0, help='é™½æ€§æ—¥æ•°(is_vcc=True)ã®ä¸Šä½Nä»¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ€§èƒ½ã‚’å‡ºåŠ›ï¼ˆ0ã§ç„¡åŠ¹ï¼‰')
    parser.add_argument('--positives-metric', type=str, default='MCC', help='é™½æ€§ä¸Šä½ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½µã›ã¦å‡ºåŠ›ã™ã‚‹ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆä¾‹: MCCï¼‰')
    parser.add_argument('--plot-positives-top', action='store_true', help='é™½æ€§ä¸Šä½Nä»¶ã®æ£’ã‚°ãƒ©ãƒ•ã‚‚ä¿å­˜ã™ã‚‹')
    args = parser.parse_args()
    
    output_summary_dir = REPO_ROOT / "datasets" / "derived_artifacts" / "rq1_rq2" / "evaluation_summary_comparison"
    output_summary_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ… Aggregated results will be saved to: {output_summary_dir.resolve()}")

    # 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨å®Ÿé¨“ç•ªå·ã‚’è‡ªå‹•æ¤œå‡º (â˜…ä¿®æ­£ç®‡æ‰€)
    projects_per_model = {}
    all_exp_numbers = set()
    valid_models = []

    for model_name, base_dir in BASE_DIRS.items():
        if not base_dir.exists() or not base_dir.is_dir():
            print(f"âš ï¸ Warning: Directory for model '{model_name}' not found. Skipping: {base_dir}")
            continue
        
        print(f"ğŸ” Searching in '{model_name}' directory: {base_dir}")
        projects_in_dir = set()
        for metrics_path in base_dir.rglob("exp*_metrics.json"):
            try:
                rel_project = metrics_path.parent.relative_to(base_dir).as_posix()
            except ValueError:
                continue
            projects_in_dir.add(rel_project)
            match = re.search(r'exp(\d+)_metrics.json', metrics_path.name)
            if match:
                all_exp_numbers.add(int(match.group(1)))
        if not projects_in_dir:
            print(f"âš ï¸ Warning: No experiment directories found for '{model_name}'.")
            continue
        valid_models.append(model_name)
        projects_per_model[model_name] = projects_in_dir

    # å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã«å…±é€šã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆç©é›†åˆï¼‰ã‚’ä½œæˆ
    if not projects_per_model:
        print("âŒ Error: No valid model directories found.")
        return
        
    initial_common_projects = set.intersection(*projects_per_model.values())

    if not initial_common_projects:
        print(f"âŒ Error: No common project directories found across all specified models: {valid_models}")
        return
    if not all_exp_numbers:
        print("âŒ Error: No experiment files found.")
        return

    sorted_initial_projects = sorted(list(initial_common_projects))
    sorted_exp_numbers = sorted(list(all_exp_numbers))
    print(f"\nâœ… Found {len(sorted_initial_projects)} common project directories: {sorted_initial_projects}")
    print(f"âœ… Found unique experiments: {sorted_exp_numbers}")

    # â–¼ è¿½åŠ : å…¨ãƒ¢ãƒ‡ãƒ«Ã—å…¨å®Ÿé¨“ã§å…±é€šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé›†åˆã‚’äº‹å‰ã«ç®—å‡º
    def effective_exp(model_name: str, exp: int) -> int:
        return 0 if model_name == "Random" else exp

    per_exp_common_projects = {}
    for exp_num in sorted_exp_numbers:
        projects_with_data_per_model = {}
        for model_name, base_dir in BASE_DIRS.items():
            if model_name not in valid_models:
                continue
            eff_exp = effective_exp(model_name, exp_num)
            projects_found = set()
            for project in sorted_initial_projects:
                _, met_dict = load_experiment_data(base_dir, project, eff_exp)
                if met_dict is not None:
                    projects_found.add(project)
            projects_with_data_per_model[model_name] = projects_found

        if projects_with_data_per_model:
            per_exp_common_projects[exp_num] = set.intersection(*projects_with_data_per_model.values())
        else:
            per_exp_common_projects[exp_num] = set()

    global_common_projects = set()
    non_empty_sets = [s for s in per_exp_common_projects.values() if len(s) > 0]
    if non_empty_sets:
        global_common_projects = set.intersection(*non_empty_sets)

    use_global_common = len(global_common_projects) > 0
    if use_global_common:
        sorted_global_common = sorted(list(global_common_projects))
        print(f"âœ… Using a fixed common project set across all experiments: {len(sorted_global_common)} projects")
    else:
        print("âš ï¸ No global intersection across all models and experiments. Falling back to per-experiment intersections.")

    # 2. å®Ÿé¨“ç•ªå·ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ãƒ»å¯è¦–åŒ–
    for exp_num in sorted_exp_numbers:
        print(f"\n{'='*25} Processing Experiment {exp_num} {'='*25}")

        # â–¼ å¤‰æ›´: å…¨expã§åŒä¸€é›†åˆã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ã¿å¾“æ¥å‡¦ç†ï¼‰
        if use_global_common:
            sorted_common_projects_for_exp = sorted_global_common
            print(f"  -> Using fixed {len(sorted_common_projects_for_exp)} projects for all experiments.")
        else:
            # å…±é€šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Œmetrics ãŒã‚ã‚‹ã“ã¨ã€ã®ã¿ã§åˆ¤å®šï¼ˆimportances ã¯ä¸è¦ï¼‰
            projects_with_data_per_model = {}
            for model_name, base_dir in BASE_DIRS.items():
                if model_name not in valid_models:
                    continue
                projects_found = set()
                eff_exp = effective_exp(model_name, exp_num)
                for project in sorted_initial_projects:
                    imp_df, met_dict = load_experiment_data(base_dir, project, eff_exp)
                    if met_dict is not None:
                        projects_found.add(project)
                projects_with_data_per_model[model_name] = projects_found

            final_common_projects = set.intersection(*projects_with_data_per_model.values())

            random_only_exp = False
            if not final_common_projects:
                if exp_num == 0 and "Random" in projects_with_data_per_model and projects_with_data_per_model["Random"]:
                    sorted_common_projects_for_exp = sorted(list(projects_with_data_per_model["Random"]))
                    random_only_exp = True
                    print(f"  -> Exp {exp_num} has only 'Random'. Using {len(sorted_common_projects_for_exp)} projects.")
                else:
                    print(f"  - Skipping Exp {exp_num}: No projects found with data for all models.")
                    continue
            else:
                sorted_common_projects_for_exp = sorted(list(final_common_projects))
                print(f"  -> Analyzing {len(sorted_common_projects_for_exp)} projects for this experiment: {sorted_common_projects_for_exp}")

        all_models_importances_list = []
        all_models_metrics_list = []
        
        for model_name, base_dir in BASE_DIRS.items():
            if not base_dir.exists():
                continue
            if not use_global_common:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã® random_only_exp ã¯ç¶­æŒ
                if 'random_only_exp' in locals() and random_only_exp and model_name != "Random":
                    continue

            importances_per_model, metrics_per_model = [], []
            projects_with_data_count = 0

            eff_exp = effective_exp(model_name, exp_num)

            for project in sorted_common_projects_for_exp:
                imp_df, met_dict = load_experiment_data(base_dir, project, eff_exp)

                if met_dict is not None:
                    projects_with_data_count += 1
                    main_metrics = {
                        'MCC': met_dict.get('mcc'), 'AUC_ROC': met_dict.get('auc_roc'),
                        'Accuracy': met_dict.get('accuracy'),
                        'Precision': met_dict.get('classification_report_dict', {}).get('class_1', {}).get('precision'),
                        'Recall': met_dict.get('classification_report_dict', {}).get('class_1', {}).get('recall'),
                        'F1-Score': met_dict.get('classification_report_dict', {}).get('class_1', {}).get('f1-score'),
                        'PR_AUC': met_dict.get('classification_report_dict', {}).get('class_1', {}).get('pr_auc')
                    }
                    temp_df = pd.DataFrame(list(main_metrics.items()), columns=['Metric', 'Value']).dropna()
                    temp_df['project'] = project
                    metrics_per_model.append(temp_df)

                if imp_df is not None:
                    imp_df['project'] = project
                    importances_per_model.append(imp_df)
            
            if importances_per_model or metrics_per_model:
                print(f"  - Found data for '{model_name}' in {projects_with_data_count} projects.")
                if importances_per_model:
                    model_importances_df = pd.concat(importances_per_model, ignore_index=True)
                    model_importances_df['model'] = model_name
                    all_models_importances_list.append(model_importances_df)

                if metrics_per_model:
                    model_metrics_df = pd.concat(metrics_per_model, ignore_index=True)
                    model_metrics_df['model'] = model_name
                    all_models_metrics_list.append(model_metrics_df)

        if not all_models_metrics_list:
            print(f"  - No valid data found for any model in experiment {exp_num}. Skipping.")
            continue
        
        final_metrics_df = pd.concat(all_models_metrics_list, ignore_index=True)
        final_importances_df = pd.concat(all_models_importances_list, ignore_index=True) if all_models_importances_list else pd.DataFrame()
        
        metrics_csv_path = output_summary_dir / f"exp{exp_num}_all_models_metrics_comparison.csv"
        importances_csv_path = output_summary_dir / f"exp{exp_num}_all_models_importances_comparison.csv"
        final_metrics_df.to_csv(metrics_csv_path, index=False)
        if not final_importances_df.empty:
            final_importances_df.to_csv(importances_csv_path, index=False)
        print(f"  - Saved aggregated metrics to: {metrics_csv_path.name}")
        if not final_importances_df.empty:
            print(f"  - Saved aggregated importances to: {importances_csv_path.name}")

        # â–¼ è¿½åŠ : ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å¹³å‡ã‚’ç®—å‡ºã—ã¤ã¤ã€é›†è¨ˆã«ä½¿ã‚ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°ã‚‚ä»˜ä¸
        mean_metrics_df = final_metrics_df.groupby(['model', 'Metric'])['Value'].mean().sort_values(ascending=False).reset_index()
        mean_metrics_df.rename(columns={'Value': 'Mean_Value'}, inplace=True)
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®é›†è¨ˆå¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰
        project_counts = final_metrics_df.groupby('model')['project'].nunique().reset_index().rename(columns={'project': 'Project_Count'})
        mean_metrics_df = mean_metrics_df.merge(project_counts, on='model', how='left')

        mean_metrics_csv_path = output_summary_dir / f"exp{exp_num}_mean_metrics_by_model.csv"
        mean_metrics_df.to_csv(mean_metrics_csv_path, index=False, float_format='%.6f')
        print(f"  - Saved mean metrics by model to: {mean_metrics_csv_path.name} (with Project_Count)")

        # 3. ä¸Šä½Nä»¶ã®æ€§èƒ½ã‚’å‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        export_top_n_performance(final_metrics_df, exp_num, output_summary_dir, args.top_n, args.top_metric, args.plot_top)

        # 3b. é™½æ€§æ—¥æ•°ä¸Šä½Nä»¶ã®æ€§èƒ½ã‚’å‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        export_top_by_positive_days(
            final_metrics_df,
            exp_num,
            output_summary_dir,
            sorted_common_projects_for_exp,
            valid_models,
            args.top_by_positives,
            args.positives_metric,
            args.plot_positives_top,
        )

        # 4. å¯è¦–åŒ–ï¼ˆç‰¹å¾´é‡é‡è¦åº¦ã¯ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ã€Random ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹å ´åˆã‚ã‚Šï¼‰
        num_projects_in_exp = final_metrics_df['project'].nunique()
        visualize_per_model_importance(
            final_metrics_df,
            final_importances_df if not final_importances_df.empty else pd.DataFrame(columns=['model','feature','importance']),
            exp_num, num_projects_in_exp
        )

    print("\nğŸ‰ Analysis complete!")

if __name__ == '__main__':
    main()
