"""
OSS-Fuzz脆弱性データ抽出ツール（OSV GCSアーカイブ版）

このスクリプトは Google OSS-Fuzz 向けに公開されている OSV のGCSアーカイブから
脆弱性情報を取得し、CSV に変換します。
"""

import argparse
import io
import json
import os
import sys
import tempfile
import zipfile
from datetime import datetime, timezone
from typing import Any, Dict, Iterator, Optional

import pandas as pd
import requests

DEFAULT_ARCHIVE_URL_TEMPLATE = (
    "https://osv-vulnerabilities.storage.googleapis.com/{ecosystem}/all.zip"
)
DOWNLOAD_CHUNK_SIZE = 1024 * 1024  # 1 MiB
REPORT_URL_KEY = "bugs.chromium.org/p/oss-fuzz/issues/detail?id="


def normalize_modified_since(raw: Optional[str]) -> Optional[str]:
    if not raw:
        return None
    normalized = raw.strip()
    if not normalized:
        return None
    try:
        dt = datetime.fromisoformat(normalized.replace("Z", "+00:00"))
    except ValueError:
        try:
            dt = datetime.strptime(normalized, "%Y-%m-%d")
        except ValueError as exc:
            raise ValueError(
                "modified_sinceはISO8601形式（例: 2024-08-01T00:00:00Z）"
                "またはYYYY-MM-DD形式で指定してください。"
            ) from exc
        dt = dt.replace(tzinfo=timezone.utc)
    else:
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")


def download_archive(
    archive_url: str, timeout: int, cache_dir: Optional[str]
) -> tuple[str, bool]:
    """GCS 上の ZIP アーカイブをダウンロードしローカルパスを返す。"""
    use_cache = bool(cache_dir)
    if use_cache:
        os.makedirs(cache_dir, exist_ok=True)
        basename = os.path.basename(archive_url) or "all.zip"
        local_path = os.path.join(cache_dir, basename)
        if os.path.exists(local_path):
            return local_path, False
    else:
        fd, local_path = tempfile.mkstemp(suffix=".zip")
        os.close(fd)

    cleanup = not use_cache
    try:
        with requests.get(archive_url, stream=True, timeout=timeout) as resp:
            resp.raise_for_status()
            with open(local_path, "wb") as fh:
                for chunk in resp.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
                    if chunk:
                        fh.write(chunk)
    except Exception:
        if cleanup and os.path.exists(local_path):
            os.remove(local_path)
        raise

    return local_path, cleanup


def iter_archive_entries(archive_path: str) -> Iterator[Dict[str, Any]]:
    with zipfile.ZipFile(archive_path) as zf:
        for name in zf.namelist():
            if name.endswith("/"):
                continue
            with zf.open(name) as raw:
                with io.TextIOWrapper(raw, encoding="utf-8") as text_fp:
                    yield json.load(text_fp)


def iter_vulnerabilities_from_archive(
    archive_url: str,
    page_size: int,
    modified_since: Optional[str],
    max_pages: Optional[int],
    timeout: int,
    cache_dir: Optional[str],
) -> Iterator[Dict[str, Any]]:
    since_dt: Optional[datetime] = None
    if modified_since:
        since_dt = datetime.fromisoformat(modified_since.replace("Z", "+00:00"))

    archive_path, should_cleanup = download_archive(archive_url, timeout, cache_dir)
    buffer = []
    page_counter = 0

    try:
        for vuln in iter_archive_entries(archive_path):
            if since_dt:
                modified_value = vuln.get("modified")
                if modified_value:
                    try:
                        modified_dt = datetime.fromisoformat(
                            str(modified_value).replace("Z", "+00:00")
                        )
                    except ValueError:
                        modified_dt = None
                    if modified_dt and modified_dt < since_dt:
                        continue

            buffer.append(vuln)
            if len(buffer) == page_size:
                page_counter += 1
                print(
                    f"アーカイブから{len(buffer)}件の脆弱性を読み込みました (チャンク {page_counter})",
                    file=sys.stderr,
                )
                for item in buffer:
                    yield item
                buffer.clear()
                if max_pages and page_counter >= max_pages:
                    return

        if buffer:
            page_counter += 1
            print(
                f"アーカイブから{len(buffer)}件の脆弱性を読み込みました (チャンク {page_counter})",
                file=sys.stderr,
            )
            for item in buffer:
                yield item
    finally:
        if should_cleanup:
            try:
                os.remove(archive_path)
            except OSError:
                pass


def extract_record(vuln: Dict[str, Any]) -> Dict[str, Any]:
    monorail_id = None
    report_url = None

    for ref in vuln.get("references", []):
        ref_type = str(ref.get("type", "")).upper()
        if ref_type == "REPORT":
            report_url = ref.get("url")
            if report_url and REPORT_URL_KEY in report_url:
                tail = report_url.split("id=")[-1]
                try:
                    monorail_id = int(tail)
                except ValueError:
                    monorail_id = tail
            break

    package_name = None
    ecosystem = None
    introduced_commits = []
    fixed_commits = []
    versions = None
    severity = None
    repo = None

    affected_list = vuln.get("affected", [])
    if affected_list:
        affected_entry = affected_list[0]
        pkg_info = affected_entry.get("package", {})
        package_name = pkg_info.get("name")
        ecosystem = pkg_info.get("ecosystem")

        versions_list = affected_entry.get("versions", [])
        if versions_list:
            versions = ";".join(versions_list)

        for range_entry in affected_entry.get("ranges", []):
            if range_entry.get("type") != "GIT":
                continue
            repo = range_entry.get("repo")
            for event in range_entry.get("events", []):
                if "introduced" in event:
                    introduced_commits.append(event["introduced"])
                if "fixed" in event:
                    fixed_commits.append(event["fixed"])

        ecospec = affected_entry.get("ecosystem_specific", {})
        db_specific = affected_entry.get("database_specific", {})

        if ecospec.get("severity"):
            severity = ecospec["severity"]
        elif db_specific.get("severity"):
            severity = db_specific["severity"]
        elif affected_entry.get("severity"):
            sev_value = affected_entry.get("severity")
            if isinstance(sev_value, list) and sev_value:
                severity = sev_value[0].get("score")
            elif isinstance(sev_value, str):
                severity = sev_value

    if severity is None:
        severity_list = vuln.get("severity")
        if isinstance(severity_list, list) and severity_list:
            severity = severity_list[0].get("score")

    introduced_str = ";".join(introduced_commits)
    fixed_str = ";".join(fixed_commits)

    return {
        "monorail_id": monorail_id,
        "OSV_id": vuln.get("id"),
        "repo": repo,
        "summary": vuln.get("summary"),
        "details": vuln.get("details"),
        "modified": vuln.get("modified"),
        "published": vuln.get("published"),
        "report_url": report_url,
        "package_name": package_name,
        "ecosystem": ecosystem,
        "introduced_commits": introduced_str,
        "fixed_commits": fixed_str,
        "versions": versions,
        "severity": severity,
    }


def main() -> None:
    here = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.abspath(os.path.join(here, "..", ".."))
    default_out_dir = os.path.join(
        repo_root, "datasets", "derived_artifacts", "vulnerability_reports"
    )
    default_out_csv = os.environ.get(
        "VULJIT_VUL_CSV",
        os.path.join(default_out_dir, "oss_fuzz_vulnerabilities.csv"),
    )
    default_archive_url = os.environ.get("VULJIT_OSV_ARCHIVE_URL")

    parser = argparse.ArgumentParser(
        description="OSVのGCSアーカイブからOSS-Fuzz脆弱性の詳細を取得しCSVに保存します。"
    )
    parser.add_argument("--out", default=default_out_csv, help="出力するCSVファイルのパス")
    parser.add_argument(
        "--ecosystem",
        default="OSS-Fuzz",
        help="取得対象のOSVエコシステム名 (既定: OSS-Fuzz)。アーカイブURL未指定時に利用",
    )
    parser.add_argument(
        "--archive-url",
        default=default_archive_url,
        help="GCS上のOSVアーカイブZIP URL (既定: エコシステム名から自動構築)",
    )
    parser.add_argument(
        "--cache-dir",
        default=None,
        help="ダウンロード済みアーカイブを再利用するためのディレクトリ",
    )
    parser.add_argument(
        "--page-size",
        type=int,
        default=500,
        help="一度にCSVへ変換するレコード数 (正の整数)",
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=None,
        help="取得するページ数の上限 (未指定の場合は全件取得)",
    )
    parser.add_argument(
        "--modified-since",
        default=None,
        help="指定日時以降に更新された脆弱性のみを取得 (ISO8601またはYYYY-MM-DD)",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=60,
        help="HTTPリクエストのタイムアウト秒数",
    )
    args = parser.parse_args()

    if args.page_size <= 0:
        parser.error("--page-size は正の整数を指定してください。")
    if args.max_pages is not None and args.max_pages <= 0:
        parser.error("--max-pages は正の整数を指定してください。")

    try:
        modified_since = normalize_modified_since(args.modified_since)
    except ValueError as exc:
        parser.error(str(exc))

    archive_url = args.archive_url or DEFAULT_ARCHIVE_URL_TEMPLATE.format(
        ecosystem=args.ecosystem
    )

    records = []
    try:
        iterator = iter_vulnerabilities_from_archive(
            archive_url=archive_url,
            page_size=args.page_size,
            modified_since=modified_since,
            max_pages=args.max_pages,
            timeout=args.timeout,
            cache_dir=args.cache_dir,
        )
        for vuln in iterator:
            records.append(extract_record(vuln))
    except requests.RequestException as exc:
        print(f"アーカイブのダウンロードに失敗しました: {exc}", file=sys.stderr)
        raise SystemExit(1) from exc
    except zipfile.BadZipFile as exc:
        print(f"ZIPアーカイブの展開に失敗しました: {exc}", file=sys.stderr)
        raise SystemExit(1) from exc
    except json.JSONDecodeError as exc:
        print(f"アーカイブ内のJSON解析に失敗しました: {exc}", file=sys.stderr)
        raise SystemExit(1) from exc
    except OSError as exc:
        print(f"ローカルファイル処理中にエラーが発生しました: {exc}", file=sys.stderr)
        raise SystemExit(1) from exc

    if not records:
        print(
            f"{args.ecosystem} エコシステムの条件に合致する脆弱性は見つかりませんでした。",
            file=sys.stderr,
        )
        return

    df = pd.DataFrame(records)
    df["monorail_id_num"] = pd.to_numeric(df["monorail_id"], errors="coerce")
    df = df.sort_values(by="monorail_id_num", na_position="last").drop(
        columns=["monorail_id_num"]
    )

    columns_order = [
        "monorail_id",
        "OSV_id",
        "package_name",
        "ecosystem",
        "repo",
        "severity",
        "summary",
        "details",
        "introduced_commits",
        "fixed_commits",
        "versions",
        "published",
        "modified",
        "report_url",
    ]
    df = df[columns_order]

    output_path = args.out
    out_dir = os.path.dirname(output_path)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    df.to_csv(output_path, index=False, encoding="utf-8")
    print(f"{len(df)}件の脆弱性情報を{output_path}に保存しました。")


if __name__ == "__main__":
    main()
