import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
import argparse

# ã‚°ãƒ©ãƒ•ã®æ—¥æœ¬èªè¡¨ç¤ºè¨­å®š (å¿…è¦ã«å¿œã˜ã¦)
# from matplotlib import rcParams
# rcParams['font.family'] = 'sans-serif'
# rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic']


## ------------------------------------------------------
## è¨­å®š
## ------------------------------------------------------
# å‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç”Ÿæˆã•ã‚ŒãŸé›†ç´„çµæœãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
BASE_PATH = Path("./evaluation_summary_comparison")  # å¤‰æ›´
COLOR_PALETTE = {
    "XGBoost": "#1f77b4",       # ãƒ–ãƒ«ãƒ¼
    "RandomForest": "#ff7f0e",  # ã‚ªãƒ¬ãƒ³ã‚¸
    "Random": "#2ca02c"         # ç·‘
}
# æ¯”è¼ƒå¯¾è±¡ã®å®Ÿé¨“åï¼ˆexp0ã€œexp5ï¼‰
EXPERIMENTS = [f"exp{i}" for i in range(0, 6)]  # å¤‰æ›´
# exp0..exp5 ã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸Šã§è¡¨ç¤ºã™ã‚‹ãƒ©ãƒ™ãƒ« (é †åºã‚’ä¿ã¤)
EXPERIMENT_LABELS = [
    "Random",
    "Kamei",
    "Kamei+Coverage",
    "VCCFINDER",
    "VCCFINDER+Coverage",
    "Coverage",
]

HUE_ORDER = ["Random", "XGBoost", "RandomForest"]


## ------------------------------------------------------
## ãƒ¡ã‚¤ãƒ³å‡¦ç† (æ”¹è‰¯ç‰ˆ)
## ------------------------------------------------------
def _fixed_ylim_for_metric(metric_name: str):
    """Return a fixed y-axis range for the given metric to avoid auto-scaling.

    - MCC: [-1, 1]
    - Others (e.g., AUC, F1, Precision, Recall, Accuracy, G-Mean): [0, 1]
    """
    name = (metric_name or "").strip().upper()
    if "MCC" in name:
        return (-1.0, 1.0)
    # Default: many common metrics are in [0, 1]
    return (0.0, 1.0)

def _collect_projects_from_df(df: pd.DataFrame):
    """Collect project-like identifiers from a DataFrame.

    Heuristically looks for columns commonly used to denote a project/repo.
    Returns a set of unique project names (strings).
    """
    if df is None or df.empty:
        return set()
    candidates = []
    for c in df.columns:
        lc = str(c).lower()
        if lc in {"project", "projects", "project_name", "repo", "repository"}:
            candidates.append(c)
    found = set()
    for c in candidates:
        try:
            for v in df[c].dropna().tolist():
                s = str(v).strip()
                if s:
                    found.add(s)
        except Exception:
            continue
    return found

def main():
    """
    ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œé–¢æ•°
    """
    print(f"--- Analyzing trends from: {BASE_PATH} ---")

    # CLI options to drive top-N visualizations
    parser = argparse.ArgumentParser(description='Visualize trends and optionally plot top-N metrics and positives-based top-N.')
    parser.add_argument('--top-n', type=int, default=0, help='å„å®Ÿé¨“ãƒ»å„ãƒ¢ãƒ‡ãƒ«ã®ä¸Šä½Nä»¶ï¼ˆç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã‚’å¯è¦–åŒ–ï¼ˆ0ã§ç„¡åŠ¹ï¼‰')
    parser.add_argument('--top-metric', type=str, default='MCC', help='ä¸Šä½Nä»¶ã®å¯¾è±¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆä¾‹: MCC, AUC_ROC ãªã©ï¼‰')
    parser.add_argument('--top-by-positives', type=int, default=0, help='é™½æ€§æ—¥æ•°(is_vcc=True)ã®ä¸Šä½Nä»¶ï¼ˆç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã‚’å¯è¦–åŒ–ï¼ˆ0ã§ç„¡åŠ¹ï¼‰')
    parser.add_argument('--positives-metric', type=str, default='MCC', help='é™½æ€§ä¸Šä½ã®å¯¾è±¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆä¾‹: MCCï¼‰')
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®ç©ºã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    all_data = []

    # å„å®Ÿé¨“ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
    used_projects = set()
    for exp_name in EXPERIMENTS:
        # èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ãŒè¨˜éŒ²ã•ã‚ŒãŸCSVã«å¤‰æ›´
        file_path = BASE_PATH / f"{exp_name}_mean_metrics_by_model.csv"

        if file_path.exists():
            df = pd.read_csv(file_path)
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãŒå«ã¾ã‚Œã¦ã„ã‚Œã°åé›†
            used_projects |= _collect_projects_from_df(df)
            # expN -> ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªåã«ãƒãƒƒãƒ—ã—ã¦å®Ÿé¨“ãƒ©ãƒ™ãƒ«åˆ—ã‚’è¿½åŠ 
            try:
                idx = int(exp_name.replace('exp', ''))
                exp_label = EXPERIMENT_LABELS[idx]
            except Exception:
                exp_label = exp_name
            df['Experiment'] = exp_label  # å®Ÿé¨“ååˆ—ã‚’è¿½åŠ ï¼ˆãƒãƒƒãƒ—æ¸ˆã¿ãƒ©ãƒ™ãƒ«ï¼‰
            all_data.append(df)
        else:
            print(f"âš ï¸ Warning: File not found, skipping: {file_path}")

    if not all_data:
        print("âŒ Error: No data files found. Please check BASE_PATH and file names.")
        return

    # å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¸€ã¤ã«çµåˆ
    combined_df = pd.concat(all_data, ignore_index=True)

    # Experimentåˆ—ã‚’ã‚«ãƒ†ã‚´ãƒªå‹ã«å¤‰æ›ã—ã€é †åºã‚’è¨­å®šï¼ˆãƒãƒƒãƒ—æ¸ˆã¿ãƒ©ãƒ™ãƒ«é †ï¼‰
    combined_df['Experiment'] = pd.Categorical(combined_df['Experiment'], categories=EXPERIMENT_LABELS, ordered=True)

    # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
    metrics_to_plot = combined_df['Metric'].unique()
    print(f"âœ… Found metrics to plot: {list(metrics_to_plot)}")

    # --- ã‚°ãƒ©ãƒ•ä½œæˆéƒ¨åˆ† (seabornã§ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚°ãƒ©ãƒ•ã«æ”¹è‰¯) ---
    for metric in metrics_to_plot:
        plt.figure(figsize=(16, 8))
        
        # å¯¾è±¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        metric_df = combined_df[combined_df['Metric'] == metric]

        if metric_df.empty:
            print(f"  - No data for metric '{metric}', skipping plot.")
            continue

        # ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¤ºé †åºã‚’å›ºå®š
        metric_df['model'] = pd.Categorical(metric_df['model'], categories=HUE_ORDER, ordered=True)

        ax = sns.barplot(data=metric_df, x='Experiment', y='Mean_Value', hue='model',
                         palette=COLOR_PALETTE, hue_order=HUE_ORDER)
        
        # ã‚°ãƒ©ãƒ•ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«ã‚’è¨­å®š
        ax.set_title(f'Comparison of Average {metric} Across Experiments', fontsize=18, weight='bold')
        ax.set_xlabel('Experiment', fontsize=14)
        ax.set_ylabel(f'Average Score (Mean {metric})', fontsize=14)

        # Xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’å›è»¢
        plt.xticks(rotation=45, ha='right', fontsize=12)
        plt.yticks(fontsize=12)

        # Yè»¸ã®ç¯„å›²ã‚’è‡ªå‹•èª¿æ•´ã—ã€å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
        if not metric_df.empty:
            min_val = metric_df['Mean_Value'].min()
            max_val = metric_df['Mean_Value'].max()
            # â–¼ ä¿®æ­£: è² å€¤ãŒã‚ã‚‹å ´åˆã¯ä¸‹é™ã‚’ã‚ˆã‚Šè² å´ã«æ‹¡å¼µã—ã¦ã‚¯ãƒªãƒƒãƒ—ã‚’é¿ã‘ã‚‹
            if pd.notna(min_val) and min_val < 0:
                y_min = min(min_val * 1.1, -1.0)  # ä½™ç™½ç¢ºä¿ã—ã¤ã¤ -1.0 ä»¥ä¸‹ã«ã—ãªã„
            else:
                y_min = 0
            y_max = max(1.0, max_val * 1.1 if pd.notna(max_val) else 1.0)
            ax.set_ylim(y_min, y_max)

        # æ£’ã®ä¸Šã«æ•°å€¤ã‚’è¡¨ç¤º
        for p in ax.patches:
            height = p.get_height()
            if pd.notna(height):
                label = f'{height:.4f}'
                # â–¼ å¤‰æ›´: æ­£è² ã§æ³¨è¨˜ä½ç½®ã‚’åˆ‡ã‚Šæ›¿ãˆï¼ˆè² å€¤ã§ã‚‚è¡¨ç¤ºï¼‰
                if height >= 0:
                    ax.annotate(label,
                                (p.get_x() + p.get_width() / 2., height),
                                ha='center', va='bottom',
                                xytext=(0, 9), textcoords='offset points',
                                fontsize=9, color='black')
                else:
                    ax.annotate(label,
                                (p.get_x() + p.get_width() / 2., height),
                                ha='center', va='top',
                                xytext=(0, -9), textcoords='offset points',
                                fontsize=9, color='black')
        
        ax.legend(title='Model', fontsize=11)
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        # ã‚°ãƒ©ãƒ•ã‚’ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        output_filename = f'comparison_trend_{metric}.png'
        plt.savefig(output_filename)
        print(f"  ğŸ“ˆ Graph saved as '{output_filename}'")
        
        plt.close()

    # -----------------------------
    # Optional: Top-N (by value) aggregated like normal plots, for ALL metrics
    # -----------------------------
    if args.top_n and args.top_n > 0:
        print(f"\n--- Building top-{args.top_n} aggregated plots for ALL metrics ---")
        topn_rows = []
        for i, exp_name in enumerate(EXPERIMENTS):
            all_metrics_path = BASE_PATH / f"{exp_name}_all_models_metrics_comparison.csv"
            if not all_metrics_path.exists():
                print(f"âš ï¸ Skipping {exp_name}: missing {all_metrics_path.name}")
                continue
            try:
                df_all = pd.read_csv(all_metrics_path)
            except Exception as e:
                print(f"âš ï¸ Failed to read {all_metrics_path}: {e}")
                continue
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãŒå«ã¾ã‚Œã¦ã„ã‚Œã°åé›†
            used_projects |= _collect_projects_from_df(df_all)
            # Determine experiment label
            try:
                idx = int(exp_name.replace('exp', ''))
                exp_label = EXPERIMENT_LABELS[idx]
            except Exception:
                exp_label = exp_name

            metrics_here = sorted(df_all['Metric'].dropna().unique().tolist())
            for metric in metrics_here:
                df_m = df_all[df_all['Metric'] == metric]
                # Per model: sort by Value desc, take top-N, compute mean
                for model in HUE_ORDER:
                    g = df_m[df_m['model'] == model]
                    if g.empty:
                        continue
                    g_top = g.sort_values('Value', ascending=False).head(args.top_n)
                    mean_top = g_top['Value'].mean()
                    topn_rows.append({
                        'Experiment': exp_label,
                        'model': model,
                        'Metric': metric,
                        'Mean_TopN': mean_top
                    })

        if topn_rows:
            topn_df = pd.DataFrame(topn_rows)
            # Plot for each metric similarly to the main section
            for metric in topn_df['Metric'].unique():
                plt.figure(figsize=(16, 8))
                sub = topn_df[topn_df['Metric'] == metric].copy()
                sub['Experiment'] = pd.Categorical(sub['Experiment'], categories=EXPERIMENT_LABELS, ordered=True)
                sub['model'] = pd.Categorical(sub['model'], categories=HUE_ORDER, ordered=True)
                ax = sns.barplot(data=sub, x='Experiment', y='Mean_TopN', hue='model',
                                 palette=COLOR_PALETTE, hue_order=HUE_ORDER)
                ax.set_title(f'Comparison of Average {metric} (Top-{args.top_n}) Across Experiments', fontsize=18, weight='bold')
                ax.set_xlabel('Experiment', fontsize=14)
                ax.set_ylabel(f'Average Score (Mean {metric}, Top-{args.top_n})', fontsize=14)
                plt.xticks(rotation=45, ha='right', fontsize=12)
                plt.yticks(fontsize=12)
                # Annotate values on bars
                for p in ax.patches:
                    height = p.get_height()
                    if pd.notna(height):
                        label = f'{height:.4f}'
                        if height >= 0:
                            ax.annotate(label,
                                        (p.get_x() + p.get_width() / 2., height),
                                        ha='center', va='bottom',
                                        xytext=(0, 9), textcoords='offset points',
                                        fontsize=9, color='black')
                        else:
                            ax.annotate(label,
                                        (p.get_x() + p.get_width() / 2., height),
                                        ha='center', va='top',
                                        xytext=(0, -9), textcoords='offset points',
                                        fontsize=9, color='black')
                ax.legend(title='Model', fontsize=11)
                plt.grid(axis='y', linestyle='--', alpha=0.7)
                plt.tight_layout()
                out = f"trend_top{args.top_n}_avg_{metric}.png"
                plt.savefig(out, dpi=150)
                print(f"  ğŸ“ˆ Saved: {out}")
                plt.close()

    # -----------------------------
    # Optional: Top-N by positives â€” aggregated like normal plots, for ALL metrics
    # -----------------------------
    if args.top_by_positives and args.top_by_positives > 0:
        print(f"\n--- Building positives-based top-{args.top_by_positives} aggregated plots for ALL metrics ---")
        pos_rows = []
        for i, exp_name in enumerate(EXPERIMENTS):
            all_pos_path = BASE_PATH / f"{exp_name}_top{args.top_by_positives}_by_positive_days_all_metrics.csv"
            if not all_pos_path.exists():
                print(f"âš ï¸ Skipping {exp_name}: missing {all_pos_path.name}")
                continue
            try:
                dfp = pd.read_csv(all_pos_path)
            except Exception as e:
                print(f"âš ï¸ Failed to read {all_pos_path}: {e}")
                continue
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãŒå«ã¾ã‚Œã¦ã„ã‚Œã°åé›†
            used_projects |= _collect_projects_from_df(dfp)
            # Determine experiment label
            try:
                idx = int(exp_name.replace('exp', ''))
                exp_label = EXPERIMENT_LABELS[idx]
            except Exception:
                exp_label = exp_name
            metrics_here = sorted(dfp['Metric'].dropna().unique().tolist())
            for metric in metrics_here:
                mdf = dfp[dfp['Metric'] == metric]
                for model in HUE_ORDER:
                    g = mdf[mdf['model'] == model]
                    if g.empty:
                        continue
                    mean_val = g['Value'].mean()
                    pos_rows.append({
                        'Experiment': exp_label,
                        'model': model,
                        'Metric': metric,
                        'Mean_TopPos': mean_val
                    })

        if pos_rows:
            posdf = pd.DataFrame(pos_rows)
            for metric in posdf['Metric'].unique():
                plt.figure(figsize=(16, 8))
                sub = posdf[posdf['Metric'] == metric].copy()
                sub['Experiment'] = pd.Categorical(sub['Experiment'], categories=EXPERIMENT_LABELS, ordered=True)
                sub['model'] = pd.Categorical(sub['model'], categories=HUE_ORDER, ordered=True)
                ax = sns.barplot(data=sub, x='Experiment', y='Mean_TopPos', hue='model',
                                 palette=COLOR_PALETTE, hue_order=HUE_ORDER)
                ax.set_title(f'Comparison of Average {metric} (Top-{args.top_by_positives} by positives) Across Experiments', fontsize=18, weight='bold')
                ax.set_xlabel('Experiment', fontsize=14)
                ax.set_ylabel(f'Average Score (Mean {metric}, Top-{args.top_by_positives})', fontsize=14)
                # å›ºå®šã®ç¸¦è»¸ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«å¿œã˜ãŸæ—¢å®šç¯„å›²ï¼‰ã§è¡¨ç¤ºã—ã€æ£æ„çš„ãªè‡ªå‹•èª¿æ•´ã‚’é¿ã‘ã‚‹
                ymin, ymax = _fixed_ylim_for_metric(metric)
                ax.set_ylim(ymin, ymax)
                plt.xticks(rotation=45, ha='right', fontsize=12)
                plt.yticks(fontsize=12)
                for p in ax.patches:
                    height = p.get_height()
                    if pd.notna(height):
                        label = f'{height:.4f}'
                        if height >= 0:
                            ax.annotate(label,
                                        (p.get_x() + p.get_width() / 2., height),
                                        ha='center', va='bottom',
                                        xytext=(0, 9), textcoords='offset points',
                                        fontsize=9, color='black')
                        else:
                            ax.annotate(label,
                                        (p.get_x() + p.get_width() / 2., height),
                                        ha='center', va='top',
                                        xytext=(0, -9), textcoords='offset points',
                                        fontsize=9, color='black')
                ax.legend(title='Model', fontsize=11)
                plt.grid(axis='y', linestyle='--', alpha=0.7)
                plt.tight_layout()
                out = f"trend_top{args.top_by_positives}_bypositives_avg_{metric}.png"
                plt.savefig(out, dpi=150)
                print(f"  ğŸ“ˆ Saved: {out}")
                plt.close()

    # è§£æã§ç”¨ã„ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§ã‚’æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤º
    if used_projects:
        print("\nUsed projects (count: {}):".format(len(used_projects)))
        for p in sorted(used_projects):
            print(f" - {p}")
    else:
        print("\nUsed projects: (no project column found in inputs)")

    print("\nğŸ‰ Analysis complete!")

if __name__ == '__main__':
    main()
